{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_selection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZREbfpTxErH"
      },
      "source": [
        "%matplotlib inline\n",
        "# Imports\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cgMB002hjuI"
      },
      "source": [
        "# Caminho do arquivo e leitura\n",
        "csvPath = \"Caminho da pasta com o arquivo com as caracteristicas\"\n",
        "df = pd.read_csv(csvPath)\n",
        "df = df.fillna(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLZ9AoPauaFh"
      },
      "source": [
        "# Normalizar os dados com Standard Scale\n",
        "x = df.drop(['ID'], axis = 1)\n",
        "x = x.drop(['Grupo'], axis = 1)\n",
        "column_names = list(x.columns)\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(x)\n",
        "df_norm = pd.DataFrame(scaled_data, columns=column_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtuB9WOtyTSM"
      },
      "source": [
        "# Criacao do modelo\n",
        "y = df['Grupo']\n",
        "model = RandomForestClassifier(criterion= 'entropy',n_estimators=1000, bootstrap = True, max_features=10,n_jobs=-1, max_depth= 10)\n",
        "model.fit(df_norm, y)\n",
        "\n",
        "# Validação do modelo com 5 folds\n",
        "scores_dt = cross_val_score(model, df_norm, y, scoring='accuracy', cv=5)\n",
        "print(scores_dt.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTrHZylWPfd8"
      },
      "source": [
        "# Score de cada um dos atributos\n",
        "print(model.feature_importances_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIbpSbj0Gt-T"
      },
      "source": [
        "# Tabela das importancias\n",
        "feature_importances = pd.DataFrame(model.feature_importances_*100,index = df_norm.columns,columns=['Importância (%)']).sort_values('Importância (%)', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qVyRw2jH6UH"
      },
      "source": [
        "# Selecao de caracteristicas com base em um criterio\n",
        "df_eq = feature_importances.round(3)\n",
        "indexNames = df_eq[(df_eq['Importância (%)'] <= 0.4)].index\n",
        "df_eq.drop(indexNames , inplace=True)\n",
        "#print(df_eq)\n",
        "\n",
        "#Salva as caracteristicas selecionadas em um arquivo CSV\n",
        "df_eq.to_csv(\"Caminho da pasta para salvar o CSV\", index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC9K5hxaUX6N"
      },
      "source": [
        "# Mostra o nome da feature e a porcentagem de importancia\n",
        "features = column_names\n",
        "features_importance = zip(model.feature_importances_, features)\n",
        "for importance, feature in sorted(features_importance, reverse=True):\n",
        "    print(\"%s: %f%%\" % (feature, importance*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q87x5REkHBn4"
      },
      "source": [
        "#constroi um grafico de barras com as 30 primeiras caracteristicas\n",
        "graf = feature_importances.head(30).plot(kind='bar')\n",
        "graf.figure.savefig('Caminho da pasta para salvar o arquivo de imagem', dpi=300, bbox_inches = \"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCrI0CjzT4E"
      },
      "source": [
        " **Funções que podem ser utilizadas adicionalmente**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7My_tYX_WQ11"
      },
      "source": [
        "#Selecao da melhor combinação de parametros para o modelo\n",
        "param_grid = {\n",
        "            \"criterion\": ['entropy', 'gini'],\n",
        "            \"n_estimators\": [50, 75, 100],\n",
        "            \"bootstrap\": [True],\n",
        "            \"max_depth\": [10],\n",
        "            \"max_features\": ['auto', 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"accuracy\")\n",
        "grid_search.fit(df_norm, y)\n",
        "\n",
        "model = grid_search.best_estimator_ \n",
        "grid_search.best_params_, grid_search.best_score_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOkwCQImydck"
      },
      "source": [
        "# Importancia\n",
        "importance = model.feature_importances_\n",
        "\n",
        "# Sumarizacao da importancia de cada caracteristica\n",
        "for i,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
        "\n",
        "# plot da importancia de cada caracteristica\n",
        "pyplot.bar([x for x in range(len(importance))], importance*100)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxHViIulmYBK"
      },
      "source": [
        "# Heatmap/Matriz de correlação\n",
        "fig, ax = plt.subplots(figsize=(18,14))\n",
        "sns.heatmap(df.corr(),ax = ax)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
